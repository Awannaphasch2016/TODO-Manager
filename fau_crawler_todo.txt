#=====================
#==NOTE
#=====================


from 2020-9-1, until 11-1-2020, there are 27197 subreddit.

twitch title
    make a data crawler from reddit and twitter. I provide url to the github if you are intereste https://github.com/Awannaphasch2016/FAUCovid19

Goal: figure out a way to add data from stream directly to sqlite database.
what is the workflow? Can I reused old function?

    here> how to add sentiment while streaming data from reddit?
        how to modified stream data?

how to choose what operational database to use?
    1. structured data.
    2. volum -> I think i need in the gigabyte range. (account for twitter
        crawler)
        what much data is 1 million per day in term of bytes?
            megabyte, gigabyte, terabyte, and petabytes scales.
    3. performance issues via unpredictable queries -> I don't know what
    mine belong to.
        find example that is similar to my case.
            what app both write/read data real time?
                ?online store?
    4. noSQL
        MongoDB vs Apache vs Cassandra vs Redis

What will data be used for?  (data usecase)
    1. data will be provided through api (in a real-time fashion)
    2. data will be use to construct graph.
    3. data will e use to do analytics.

database dsign option
    database design option 1
        json (easy to write and hard to read )
        -> Document database (noSQL) (easy to read and hard to write.)
        -> APi
            ?what is the property of API that have access to database?

    database design option 2: using SQL datbase that support columns for
        JSON format (and also allow you to make special operations over them)
        Note: one of the SQL database option is PostgreSQL
        json
        -> stored it in PostgresSQL

    database design option 3: using both SQL and noSQL
        NOTE: consider prices for each solution that can be done using this
            option.

        NOTE: the reason for this option is because I am not sure whether or
             not when having or data from other source, I will need to use
             SQL operation. (the purpose of SQL operatio is just making it
                 easy to access, read, write, combine data )

         1. using SQL as a single source of truth ( act like data warehouse)
         2. using noSQL as a data lake

         we need microservice that ca do the following.
            ? how can each component be combined?

            > stream processos/message broker
                producer -> stream processor -> cosumer
                eg.
                    Apache Kafka and Amazon Kinesis Data Streams
            > ETL tools for straming data.
                eg.
                    data lake ETL
            > serverless query egine.
                eg.
                    lambda?
            > streaming data storage.
                database or data warehouse
                    PostgresQL or Amzaon Reditshift
                message broker

        AWS services stack option 1
            AWS CloudFormation Template  (stake things together)
                [
                    json output from api ->
                     Kenesis ->
                     [
                      Amazon S3 (data warehouse) ->
#                       Amazon DynamoB (noSQL database) ->
                      AWS lambda (create function that response to api request.
                           to query) ->
        #               AWS GLUE (data transfomration: this is ETL, so I need to
        #                   use it with data warehouse ) ->
        #               AWS Athena (sql engine)
                     ] (both things can be access with data lake command line)->
                      provide API

                    what is the ETL technology that I am looking for?
                ]
        <selected> AWS services stack option 2
            Kinesis data stream->
                1. Kinesis firehorse
                    can I apply sentiment analysis in this process? if not
                    selected the following option
                        1.1 use DynamoDB Stream -> trigger lambda -> S3
                        1.2 How to apply sentiment analysis to Kinesis
                                see Real-Time Reddit Streaming Solution: Self-Guided Tutorial

                   S3 ->
                   query "aspect" straight from S3 (in batch)

                   If query from s3 is not possilbe, add to DynamoDB and
                   query from it instread
                   DynamoDB
                    given primary key&sorted kyes
                        option1
                            sorted keys = subreddit
                            primary key = comment/submission
                        <selected >option2
                            primary key = subreddit
                            sorted_key = date time

     database design option 4: use Amazon microservice
        ?what are microservices that I will need?
            microservice for ETL? or real-time (like kafka)
     database design option 5: using Docker Container


    using EC2 with lambda
    using docker with lambda

    read worign with AWS Lambda and Lambda Layers in AWS SAm
        what is Lambda Lyaer

    here> hwo to import Custon Python pKacges on AW Lambda Funciton
        https://www.youtube.com/watch?v=yyBSeGkuPqk&ab_channel
        =Tekashi6ix9ineTekashi6ix9ineOfficialArtistChannelk:w

    SEt up locla lambda Developement Envionrment Using SAM _
        https://www.youtube.com/watch?v=bih5b3C1nqc&ab_channel=Hip-HopUniverseHip-HopUniverseVerified

    read Workgin with AWS Lambda and Lambda Layers in AWS SAM
       read https://aws.amazon.com/blogs/compute/working-with-aws-lambda-and-lambda-layers-in-aws-sam/

        how to create docker container for Amazon Linux?
            Configuring a function to use layers
            Managing layers
            Inlcuidng library dependencies in a layer
            layer permissions
            AWS CloudFormation and AWS SAM
            SAmple application

    Develope AWS Lambda with AWS EC2 (or Docker)
        read hwo to use AWS Lambda service with EC2-an Example
            https://learnfromsanthosh.com/how-to-use-aws-lambda-service/



    read Buildig application
        https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-using-build.html

    ? if you are creating a deployment package used in a layer.
        read including library dependencies in a layer
           https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html#configuration-layers-path

    with a virtual environment
        https://docs.aws.amazon.com/lambda/latest/dg/python-package.html

API Gateway

    ? what is REST Headers and Paprametres?
        https://www.soapui.org/learn/api/understanding-rest-headers-and-parameters/
    AWS X-Ray?
    AWS WAF?

What is AWS data pipeline?

kubectul config current-cotext


system requirement
    store data before modified.
    store data after modified.
    store missing data when client side fail
    store missing data when server side fail
    stream data as well as stored data can be accessed via url endpoint
    system add sentiment to the stream data

user requirment
    user can get streaming data from URL endpoint
    user can specified date range, crawler_type, aspect,
    user can use the same url paramters .


#=====================
#==QUESTION
#=====================
How does retrying work in psaw?

When does generator return every repeat? or after all data is retreieved?


things related to AWS
    Is there a way I can use AWS at cheaper price with student status?
        what is AWS Educate?


    what is .rest file?
        @api = url to api

        POST {{api}}
        content-type: application/json
        {
        "text": "str"
        }

    what is ELB
    what is Auto Scale
    what is Aamzon Neptune?
        Amazon Neptune vs Neo4j
    what is sns and cds?

    what is this Amazon ECS (deploy)?
    AWS fargat vs AWS Lambda
        AWS fargate
            container definition
            task definition
            Service
            Cluster

    how policy works?
    how role work?
    when to use stream vs analytics vs  firehose?
    s3
        what is folder vs bucket?


    what is Apache Parquet?
    what is Apache ORC?

    read and understand referece link in Real-Time Reddit Straming Solution
    Self-Guided

what is ELB health check?


#=====================
#==WAITING
#=====================

#=====================
#==OPTIMIZATION
#Note: no need to be done right now, but if it is done it will be better.
#=====================
Optiomize: look at the data in database and add
query_word as necessary

c:\users\anak\appdata\local\programs\python\python38\python.exe -m pip install --upgrade pip'
#=====================
#==TODO
#=====================

correlation technique (what are their assumption?)
    spearman correlation
    pearson correlation
    kendall correlation

word cloud
    word token
    symptom token
    group token

symptom that cooccurs together
correlation between words

what is mapping tempalte?
    ref: https://www.alexdebrie.com/posts/api-gateway-elements/#roadmap-the-three-basic-parts

what is backoff?

read  "consuming streaming data" post from twitter
    url: https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data#disconnect_messages

task:

    how to get sentiment score form dynamoDB?
        sol 1
            send data to dynamoDBStream and connect APIgateway to it.
                https://medium.com/swlh/skip-lambda-save-data-to-dynamodb-directly-using-api-gateway-process-later-with-streams-dab2ceef9a9d
        sol 2 
            can dynamoDB stream be used as trigger?
        sol 3 
            here> get data from dynamoDB directly
                do i need KCL to do dynamoDB + kinesis stream?
        sol 4
            lambda -> kinesis firehose -> api gateway (processed_stream)

    send zhabiz the code snippet
        figure out a wwa to deal with output
    here> provide more api parameters and path .
        get sample from aspect
        get lastest/earilest N results
        get top/bottom n sentiments
    try connect data to reddit crawler

    how can I create group of aspect?
        sol1 
            aspect=covid -> a group of query result for 'corona, covid19, ...'
                figure out a way to push lambda
                    cacher

solution1
    setup api gateway to run lambda. in this case I don't even need ec2.
        how do I upload lambda?
            impement python to run script
                how the fuck do i even write the damn script.
                can I do it without script?
            here> using SAM.
                run sam to see what is the expected output. and whether it
                 is something that will be useful for the current task?
                Will lambda run forever?
                How to stop lambda?
                how the fuck do i test this?
                    here> why deploy fail?
                        where do i add policy and which policy do i need ot
                         add?
                            add create permission to template.yaml
                                how to add policy to Resources in template.yaml
                            how do I know list of role I can use?

Goal: replicate current API gateway that I have.
    here> create path for dynamoDB stream
        here> figure how to create path and method
        figure how to connect to kinesis
        figure how to connect to dynamoDB
        figure how to connect to s3


using cdk with api


    CDK for serverless.
        here> running lambda on EC2
            how to load lambda into EC2 using python?
                solution attempt

                using web server (how?)
                    " Python-based web server on the EC2 box itself (e.g. Flask). "
                using ssh (how?)

        here> create path for dynamoDB stream
            figure how to create path and method
            figure how to connect to kinesis
            figure how to connect to dynamoDB
            figure how to connect to s3

    Note:
        api invoke url
             https://om3ozpzn1e.execute-api.us-east-2.amazonaws.com/test
        example
             curl -v -X POST 'https://om3ozpzn1e.execute-api.us-east-2.amazonaws.com/test/helloworld?name=John&city=Seattle' -H 'content-type: application/json' -H 'day: Thursday' -d '{ "time": "evening" }'
             curl -v -X GET 'https://om3ozpzn1e.execute-api.us-east-2.amazonaws.com/test/helloworld?name=John&city=Seattle' -H 'content-type: application/json' -H 'day: Thursday' -d '{ "time": "evening" }'
             curl -v -X PUT 'https://om3ozpzn1e.execute-api.us-east-2.amazonaws.com/test/helloworld?name=John&city=Seattle' -H 'content-type: application/json' -H 'day: Thursday' -d '{ "time": "evening" }'

predict positive or negative using either decision tree, svm, rf, lr



here> lets make things work (using the least tools)
    here> Goal: get stream data from api
        no code
            demo
                here> api gateway with kinesis vs kinesis endpoint?
                    here> api gateway with kinesis

                        requirement
                            user requirement
                                List the user's aviable stramsi nKinesis
                                create, desecibe or delete a specified stram
                                    NOte: I don't really eed this one
                                read adata record form or wirte data record into the
                                    specified stream
                            system requirement
                                api exposes methods on various resources to invoke the
                                 following
                                    note: all CRUD are determined by methods avialbe in
                                     kinesis
                                        VAlidate: what are the availbe method in
                                        kinessi?
                                            see API/Actions
                                    1. ListSTream action in Kinesis
                                       GET + ListSTream -> /streams

                                    2. CreateSTream, DescribeStream, or DeleteStream

                                       POST + createStream -> /streams/{stream-name}

                                       GET + DescribeStream -> /streams/{stream-name}
                                        describe kinesis stream

                                       DELETE + DeleteSTream -> /streams/{stream-name}

                                    3. GetRecords or PutRecords
                                       PUT + PutRecord ->
                                       /stremas/{stream-name}/records
                                        enable the client to ass a single data record
                                            to the named strema
                                        enable the client to ass lsit of data record
                                            to the named strema
                                       GET + GetRecord ->
                                       /stremas/{stream-name}/records
                                        get list of data with a specified shard
                                        iteration
                                            note:
                                                shard iterator specified the shared
                                                positio nform which to start reading
                                                data records sequentially.
                                            Validate:
                                                will this work when timesteamp is
                                                already past or in the future?
                                       GET + GetShardIterator ->
                                       /stremas/{stream-name}/sharditerator
                                        this helper method must be suppleid to the
                                        ListSTreams action in Kinesis
                                            Validate: not sure how this will work



                    resources
                        tangible
                            Controlling Access to Amazon Kinesis Data Streams Resources Using IAM
                                https://docs.aws.amazon.com/streams/latest/dev/controlling-access.html
                            principal_block can only be in 'resource-base policy'

                        intangible

                    step
                        api permission for kinesis
                            where to add principal permission?
                        create an IAM role and policy for the api to
                         access kinesis
                        start to create an api as a kinesis proxy
                        List strams in Kinesis
                        here> create, desciribe, and delte a stram in Kinesis
                        get record from and add record to a stram in Kinesis
                            where to find shardId in kinesis stream?

                create demo for OpenAPI + import OpenAPi to API Gateway


        all use code
            Goal: AWS CDK Tutorial Python | Create Dynamodb, Kinesis, S3, Lambda, API gateway - Demo
                here> use code to do api gate way similar to what I did in console.
                    here> what is paginate operation?
                    implement twitter stream in kinesis and with apigateway

                    Boto3 and cloudformation vs cli and couformation
                        reference: https://www.bing.com/search?q=python+apigateway+kinesis&go=Search&qs=ds&form=QBRE
                        here> test the existing api gateway that I have .
                            here> GET + /stream-name/

                    select tutorial that match my case.
                        (kinesis + api gateway + lambda + s3 + dynamoDB)
                            here> kinesis + apigateway
                            s3 + api gateway
                            dyamoDB + api gateway

                    select template that I will need.
                    how to create template from AWS CLI?
                    use cloudfront to create gateway + kinesis.
                        look at cloudfront tutorial.
                use code to do api gate way focus on query string


                here> demo
                    here> new1
                        here> s3 trigger -> lambda -sentiment analyzis-> to
                        dynamoDB and to kinesis (instead of using another kinesis can I just use dynamoDB stream; what is the restriction?)
                            here> implement lambda to output original data from
                             s3 + sentiment and deliver to dynamoDB
                        create apigateway for dynamodb
                                here> deliver dynamoDB from lambda

                            stream/dynamoDB/twitter-> dynamoDB table = faucovidstream_twitter_with_sentiment

                        here> create apigtae for dynamodb
                            user requiremet
                                user can

                                /stream/dynamoDB/twitter?platform=<platform>&
                                .since=<12312124>&until=<125125>
                                    value of since and until is 'int'

                                here> query base on text
                                query things from certain date (since)
                                query things from certain date to
                                    certainr date (since-until)

                                    what are the set of keys tht i will need to
                                     complete the task?
                                        KeyConditionExpression
                                            hierarchy
                                                primary
                                        FilterExpression
                                        ProjectionExpression
                                        ExpressionAttributeValues

                                    fix so that platform is also option.
                                        platform=twitter&aspect=covid&since=1607966426788&until=1607966426788




                        here> create apigateway for s3
                            user requirment


                        how to decrypte data from api gateway.

                        I fix the following
                            1. fail and retry works
                            2. string query works for kinesis
                            --todo tmr---
                            3. string query from s3 works (past data)
                            4. string query from dynamoDB works (past data)
                            5. string query from dynamoDB stream (or kinesis) with sentiment

                        codedeploy use to rerun failed code

                    note:
                        no need for string query yet
                    api gateway with kinesis vs kinesis endpoint?
                    using boto3 for api gateway ( no need for cloudformation and CDK)

                    implelment lambda with dynamoDB.
                        note: I remember that dyamoDB + (lambda + s3 trigger) can be replaced
                            with kinesis endpoint or something
                    store data to dynamoDB withoout sentiment added
                    query aspect from dynamoDB



    Goal: lets gather more data from stream using kinesis
        how to deal with when code throw exception and stop running.
        figure out how to re-run when code throw exception
            Code Pipline vs DataPipelin vs Airflow
                if use Code Pipeline use CDK
                    see the following
                        https://docs.aws.amazon.com/cdk/latest/guide/cdk_pipeline.html
                        https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html

    connect S3 + lamba + kinesis together
cdk shit

    https://GUID.execute-api-REGION.amazonaws.com/prod/
    https://810414031677.execute-api-us-east-2.amazonaws.com/prod/
    curl -X GET 'https://GUID.execute-api.REGION.amazonaws.com/prod'
    curl -X GET 'https://810414031677.execute-api-us-east-2.amazonaws.com/prod/'

    where do I specify my srucirty token in cdK?
        do I need to provide access token to cdk when using it?
            icluding the following
                arn
                url
                access
                UID
    do I have to have bucket before that using s3.Bucket?
    how do I debug CDK?
        are there log?
            if yes, where is it?
    before trying to make MyWidgetService work
        I have to implement AWS API by itself.

    here> Goal: implement CDK as a base template for crawler
        here> CDK
            find free template for lambda, s3, kinesis.
                ?is the following option what I want?
                    option1: cdk for serverless
                        here> https://docs.aws.amazon.com/cdk/latest/guide/serverless_example.html
                            bootstrapping?
                            assets?
                            CDK pipelines

                    option2: SAM CLI with CDK
                        https://docs.aws.amazon.com/cdk/latest/guide/sam.html


-----------finish upto here by tmr-----------

Goal: connect all parts together including
    what are all the spearated demo that I have implemented?
        collect all demo
            1. demo - S3 + Lambda + Kinesis
                see demo code in Covid19CookieCutter\Examples\Demo\AWS_Related\TwitterStreamWithAWS\twitter_firehorse.py
        think of is there anything else I need?
            note: I haven't successfully stream original data + sentimet.
                I implemented straem for original data
                I implemented lambda to add sentiment

            if no, start put all parts together


migrate from cloud to local
    what do you need to use MinIO?
        read about MinIO
            https://docs.min.io/docs/minio-docker-quickstart-guide
            https://docs.minio.io/docs/deploy-minio-on-docker-compose
            https://hub.docker.com/u/minio/
            https://registry.hub.docker.com/r/minio/minio/
        see docker-compose.yml in TestBeyondJupyter

Final product
    implement demo streaming for reddit.
    streaming data operations: Stream Joins
        join reddit with twitter


FreeNAS in Window using virtual machine?

how to setup storage serve in my labtop


MinIO vs s3
    what is the difference?

read Converting Your Input Record Format in Kinesis Data Firehose.
    https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html

what is the kinesis firehose data format that is sent to S3.
    what is the aws_access_key_id and aws_secret_access_key for Zhabiz?
    how to figure this out?

better Python testing: how to mock AWS
    https://www.youtube.com/watch?v=11Fr0wqcxRc&ab_channel=JeffreyNess


Git Action vs Travis CI
    https://knapsackpro.com/ci_comparisons/github-actions/vs/travis-ci
    https://dev.to/csgeek/github-actions-vs-travis-ci-epd

type of test in software testing
    https://www.atlassian.com/continuous-delivery/software-testing/types-of-software-testing

what are the types of testing in CD/CI?
    Continuous integration vs. continuous delivery vs. continuous deployment
        https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment
    Automated Unit Testing in DevOps Pipeline | AWS Lambda Unit Testing using AWS CodeBuild
        https://www.youtube.com/watch?v=wUvYTmTvmko&ab_channel=AgentofChange
    AWS re:Invent 2019: Continuous delivery to AWS with GitHub Actions (DOP322-S)
        https://www.youtube.com/watch?v=KJNj37ZXPqE&ab_channel=AWSEvents


here> how to test AWS service in python?
    read this https://towardsdatascience.com/how-i-write-meaningful-tests-for-aws-lambda-functions-f009f0a9c587
    implement the following + add to cacher
        https://roamresearch.com/#/app/AdaptiveGraphStucture/page/lKMbKrTRO

AWS security credentials
    AWS credentials
        canoical user ID?
            when to use canoical user ID?
    AWS account identifiers
        https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html

getting started in CDK.
    read https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html
    implement CDK with Kinesis + API Gateway + Lambda + DynamoDB

#----------
begin implementing API gateway + test (using TCR)

what is the different betwen AWS CDK vs AWS SAM?
    AWS SAM -> set up everything related to serverless
        lambda + api gateway + EC2
    AWS CDK -> superset of AWS SAM?

Demo for dr Zhu before 2pm
    here> how to parse object form s3? see query_json_from_s3
        test output of tweeter stream
        what is twitter strema output format + type?
    How to grant resource (s3) access to Zhabiz?
        Policy?
        Role?
        New IAM?

API Gateway
    Tutorial: Create a REST API as an Amazon Kinesis proxy
    what are the differences between the follwoing?
        Buil an api awith Lambda integration
        ?Tutorial: Build an API with private integration.
        ?Tutorial: Build an APi with AWS integration
    Tutorial: Calc API with three integrations
    Tutorial: Create a REST API as an Amazon S3 proxy in API Gateway
    tutorial: create a REST API  by importing an example
    Build an API with HTTP integration.

do the following
    1. change from AWS ES to AWS Kinesis streawm.
        option1
            here> stream1 -> [firehose -> stream3]
                stream1 = faucovidstream_input
                firehose = faucovidstream_transform
                stream3 = faucovidstreamsentiment

                here> change from fau_covid_stream (firehose) to
                    stream1 -> firehose (faucovidsteram_transform)
                    1. convert from  fau_covid_stream to
                        faucovidstream_input -> faucovidsteram_transform
                            create demo for faucovidstream_input
                                create faucovidstream_input

        option2
            faucovidstream_transform -> add to s3 -> return result -> HTTPS
             endpoint
                implement firehose with HTTPS endpoitn
                    https://aws.amazon.com/blogs/big-data/stream-data-to-an-http-endpoint-with-amazon-kinesis-data-firehose/

                here> implement firhose with lambda transofmraiton
                    https://aws.amazon.com/blogs/compute/amazon-kinesis-firehose-data-transformation-with-aws-lambda/

                here> how to get data from the HTTPS endpoint?
                    watch youtube video.
                I can test the result by added result result to another new s3
                bucket.

    2. implement another program to call Amazon Kinesis stream and output data.
        what is the function I have to use?

here> lets show sentiment of covid19 on Kibana?
    finish the article

hwo to use dynamoDB with kinesis?
    working with streasm?
        NOTE: process stream recording using AWS Lambda, Kinesis DAta
            Analytics, Kiesis Data Firehose, or AWS Glue Streaming ETL.

        Working with Kinesis Data ?
        Workign with DynamoDB streams
            change data capture for dynamoDB streams
                https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html
            Using the DynamoDB Streams Kinesis Adapter to Process Stream Records
                https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.KCLAdapter.html
                    what is Kinesis Client Library (KCL)?
                    what is DynamoDB steram kinesis Adapter?


query by primary key
query by scan

aspect1
    subreddit1
    subreddit2
    subreddit3
    subreddit4

aspect2
    subreddit2
    subreddit4
    subreddit5

aspect3
    subreddit1
    subreddit2
    subreddit6


Goal: aspect -> if subreddit is in aspect's subreddit name list.

how do I optimize for query read time
    how should I design the reddit and twitter response key-value?


create dynamoDB database
    create dynamoDB table
    what is the key-value that I will be using?
        <selected >option2

            primary key = subreddit
            sorted_key = date time



is serialized object bytes object?
what is data blob agian?
what should I use as paritionkey?

<selected>
    input -> kinesis stream -> firehose1 -> s3 -> lambda -> sentiment on text
     ->
    dyamoDB
        note: maybe i need to change input source from 'direct put' to
         kinesis stream
            what does direct put mean for firehose?
            how is direct put implement in python code?


    consumer -> API Gateway -> lambda -> kinesis stream2 <- firehose2 <-
    kinsesis stream
        www.something.com/?param1=vale1&param2=value2...


here> using Kinesis with reddit api.
    Goal: use kinesis firehorse with reddit api.
        from steramign data to covid19 twtiter analsysi: using AWs Lambda,
            kinesis firehouse and elasticsearch
                https://towardsdatascience.com/from-streaming-data-to-covid-19-twitter-analysis-using-aws-lambda-kinesis-firehose-and-b71b71279335
                    Goal make twitter + kinesis + lambda + S3 works together.
                        fau_covid_stream-2-2020-11-30-01-13-17-15073c64-3564-4d9a-ab5a-0bc724a90777
                            what is the output firehose?

                        what is limit of kinesis per second?
                            record vs bytes
                                how much bytes is a record?

                        how to check amount of data per second from twitter?
                            can I limit to certain amount?
                        here> check why nothing is added to S3 when runnign
                        scrip.
                        what is template.yaml?
                        how to use kibana to validate twitter input stream?
                        do twitter input stream.
                            what is decdicated master node ?
                            how to network configurtion?Jk:w








    ?How to apply sentiment analysis to Kinesis
        see Real-Time Reddit Streaming Solution: Self-Guided Tutorial

<selected> AWS services stack option 2
    Kinesis data stream->
        1. Kinesis firehorse
            can I apply sentiment analysis in this process? if not
            select the following option
                1.1 use DynamoDB Stream -> trigger lambda -> S3
                here> 1.2 How to apply sentiment analysis to Kinesis
                        using Textblob
                        see Real-Time Reddit Streaming Solution: Self-Guided Tutorial

           S3 ->
           added data to S3 trigger Lambda ->
             trigger can be applied to DynamoDB stream as well.
           DynamoDB->
           query "aspect" from DynamoDB

====================

try pracing with follwoig article if necessary
    automatinng athena queries form s2 with python and boto3
        https://medium.com/dataseries/automating-athena-queries-from-s3-with-python-and-save-it-as-csv-8917258b1045
    Creating a Python Devlpement Enviornment on Amazon EC2
        https://blog.jetbrains.com/pycharm/2017/12/creating-a-development-environment-on-amazon-ec2/
    Is it possible to SSH in AWS isntances using any IDEs usch Pychamr
        https://stackoverflow.com/questions/52340973/is-it-possible-to-ssh-in-aws-instances-using-any-ides-such-pycharm
    Boto3 Doc EC2
        https://boto3.amazonaws.com/v1/documentation/api/latest/guide/ec2-examples.html

kinesis firehorse
    what is the dieliveryStreamName?

Goal: lets run comment-stream.py -> 1 am.
    read tutorial
        work on Real-Time Reddit Straming Solution Self-Guided Tutorial
            deploy the EC2 streaming server  in CloudFromation (13)

            ?Monitor the delivery stream 15)
            Use Athena to develop insights (16)
            ?clean up the environent (19)

read  Build a Serverless REal-time Data processig app
    https://aws.amazon.com/getting-started/hands-on/build-serverless-real-time-data-processing-app-lambda-kinesis-s3-dynamodb-cognito-athena/
        build a data stream
        aggregate data vs process straming data
        store & query data
        exta credit & clean up

here> how to use python to interact with S3?
    here> read the following
        Boto3
            AMAZON S3
                using an aMAZON s3 bucket as a static web host
                    ?how to get data from web host
                        I get the followig error when I go to aws web host
                        endpoint.
                            An Error Occurred While Attempting to Retrieve a Custom Error Document

                            here>when should I use bucket policy?
                            how to create bucket policy?
    pull data from S3 and group data with the same aspect together.
        S3 -> store in dynamoDB
            crawled data is added to dynamoDB with key = 'aspect'
                think of a way to regroup things together in dynamoDB so
                that it compatible with my current api endpoint.


    how do I continuously crawled data from reddit?
        other than EC2? are there better option and cheaper option

            Ec2 -> s3 -> store in dynamoDB
            OR
            Kinesis -> s3 -> store in dynamoDB


    create demo that can do the following
        1. crawled data to S3 -> dynamoDB
        2. whenever api is called query is made from dynamoDB

how to provide API endpiont using AWS service?

here> working on Demo
    work on example (if I can find example related to "work
         on the fau crawler project (step by step)", do those instead)
         Event-Driven data ingestion with AWS Lambda (S3 to S3)
            read
                https://towardsdatascience.com/building-a-data-pipeline-from-scratch-on-aws-35f139420ebc
            practical
                https://www.mydatahack.com/event-driven-data-ingestion-in-aws-s3-to-s3/#:~:text=%20Event-Driven%20Data%20Ingestion%20with%20AWS%20Lambda%20%28S3,Make%20sure%20to%20activate%20the%20virtual...%20More%20
                https://medium.com/cloudadventure/build-a-data-warehouse-and-pipelines-on-aws-f479c4472810
                https://medium.com/coinmonks/building-distributed-data-pipeline-on-aws-f5914f3f1f4f

                amazon official web
                    documentation url: https://docs.aws.amazon.com/index.html
                    Websites & Web Apps
                        Amazon EC2
                            Launch a Linux Virtual Machine
                                https://aws.amazon.com/getting-started/hands-on/launch-a-virtual-machine/
                        AWS Lambda
                            Tutorial: Deploying a Hello World application – Step-by-step instructions to download, build, and deploy a simple serverless application.
                            AWS SAM example applications in GitHub – Sample applications in the AWS SAM GitHub repository that you can further experiment with.
                                https://github.com/amazon-archives/serverless-app-examples

                    Databases
                        Amazon DyamoDB
                            create and Query a NoSQL Table
                                https://aws.amazon.com/getting-started/hands-on/create-nosql-table/

                    DevOps
                        Amazon ECS
                            deploy docker container.
                                https://aws.amazon.com/getting-started/hands-on/deploy-docker-containers/
                        Amazon EBS + Amazon EC2 + VPC
                            set up a Jenkins Build server
                                https://aws.amazon.com/getting-started/hands-on/setup-jenkins-build-server/



                    https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-creating-pipelines.html
                    https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-console-start.html




    work on the fau crawler project (step by step)
        stream reddit api
            how can I stream without using my labtop.
                use EC2.
        load it to S3 (
        everytime api is called, transform data to appropriate format
            use lambda (create function that response to api request.
                           to query)
            OR
            s3 -> lambda -> AWS Athena
                https://blockgeni.com/automating-data-using-amazon-athena-and-aws-lambda/#:~:text=Two%20Lambda%20functions%20are%20triggered%20on%20an%20hourly,the%20Athena%20CREATE%20TABLE%20AS%20SELECT%20%28CTAS%29%20query.
            OR
            S3 -> AWS Glue -> AWS Athena



    Article
        How to deploy a Docker Container on AWS.
    here> how to create stack in aWS?
        here> where do I start?
            here> read 2 article on how create stack on aws?
                goal: to see how people do it.

            github base project (demostrate how things)
            Udemy
            free youtube tutorial
            tutorial in AWS itself
        do i need to use Python?


what is free lab? (hads-on practice)
    read https://aws.amazon.com/training/intro-to-aws-labs-sm/

check out free tier
    data pipeline
        https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc
    machine learning
        https://aws.amazon.com/campaigns/free-machine-learning-on-AWS/



how to manage cost using  AWS service?

Goal is to understand components that are used to constructed each
  technology exmples

    How can I combine 3 components into 1 architecutre
        1. stream processors
        2. ETL tools for streaming data
        3. serverless query eginer.

read the following
    http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html
    https://github.blog/2009-10-20-how-we-made-github-fast/

best practices for developeing an ELT pipeline
    https://chartio.com/blog/best-practices-for-developing-an-elt-pipeline/

?streaming service vs ETL?

snoreflake
    url: https://docs.snowflake.com/en/user-guide/intro-key-concepts.html#:~:text=The%20Snowflake%20data%20warehouse%20uses%20a%20new%20SQL,but%20also%20has%20additional%20functionality%20and%20unique%20capabilities.
    database storage
        sonwflake manages all aspects how this data is toerd
            metadata
                statististic
                organization
                    file
                        structure
                        compresion
        NOTE: data object can only be accessed by using SQL query

    query processing
        queris is processe i vitual warehouses.
        each virtul warehouse is a MMP compute cluster (containing many
            compute ondes)
        each virtual warehous is indeepedet comptue cluster that does not
            share  compute rsources with other virtual warehouses.
            see vritual datawarehouse for more detail
                What is the need for virtual datawarehouse?
                what is the benfit of using i.


    cloud services.
        insfratsturure mangemnt
            what is this?
        metadata management
            what is this?


my database properties shoudl have the following components
    metadata manager.?
        what is qualified as "maanger"?


what are the type of database?
    how many ways can database be categorized?
    watch video on types of database.
        read article from the videos to understand variaable that need to
        take into account when design database as well as architecture
        design for database.

        here> take a look into database design for streaming data.
        I want to have the following properties for my design.
            unstructure data -> data warehouse
            data from data warehouse can be groupped given certain contraint,
            (queried?) -> what is the database I should use here.

        How to interact between 2 types of database
            Thigns to look for.
                use case

                    ?are there any example for us to have a look and compare
                        with my curretn situation?
                    ?in which case should I consider using more than 1
                        datbase type?
                        what is the pro and con using more than 1 databse
                        type compare to jsut 1 type.
                    ?what are the added constraint (potential problems) that I
                    have to consider?
                        any bottleneck?

            select document database
                which of the followign option should I select?
                    Option: MongoDB, DynamoDB
                    the criteria to select.
                        popularity
                        pro and con
    what is the properties of data streaming ?
1. check database persistence  levle of sql ()
    here> what are the sql option?
        here> MySQL
            practice reading the PEP.
                learn about the database programming in python
                (HigherLeverlDatabaseProgrammig)
                    ORMs vs SQL Wrapper & Geerators vs relatioal Python

                lets lewarn how to navigate trhough the wiki python.
                    what is wiki python?
                        python wiki vs documentaton?
                        are there other wiki or other langugage?
                what is required in DATABASE api specification in Python?
                    Introduction

                        ?what type of database this PEP apply to?

                    what is module iterface?
                        Constructor
                        Globals
                        Exceptions
                    Connection Ojbect
                    Cursor object.
                    Optional BD API Extesnion


            read introduction of Pytho conenctor?
            here> how to use mysql with python
                here> run mysql server.
                    here> what is mysql-connector-python username and password?
                    why can start sql server?

                        check log


    what are the nosql option?
2. change from database -> text file (and send Zhabiz unique key to filter it out.)

here> work on pulling stream data

    implement streaming
        for reddit.
        figure out why the following error occurs
            prawcore.exceptions.RequestException: error with request HTTPSConnectionPool(host='www.reddit.com', port=443): Max retries exceeded with url: /api/v1/access_token (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002A46173D8E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))


        here> get sentiment for the current dataset that I have on the fau
         lab computer

            here> fix update_sqlite3_database so that I can update
            reddit_stream.

                update data to support the following

                    raw_reddit_stream (reddit_stream)
                    processed_reddit_stream_for_api (reddit)
                        add aspect =
                        add search type = "comment"
                        add crawler = "reddit_stream"
                        add frequecy = "stream"

                    get database schema

                    select id
                    from reddit_stream
                    where id = 'gcgq7t5';

                    select *
                    from reddit
                    where id = 'gcgq7t5';


                here> remove data's keys garantee
                [x] run update_sqlite3 ->
                [] run api ->
                [] write test for all the cases

                what is the different between reddit vs reddit_stream fields
                    what are the acceptable fields for api?
                    make adjustment to function so that reddt_stream only
                        accept fields available from reddit_stream and not reddit



         skip> use multithread to run setiment analysis while stream is
         going on ?

            here> find an example that use sqlite in multi-threading
                NOTE: threading mode canbe selected at
                    1. compile-time (when the SQLite library is beign
                    ocmpiled form source code)

                    2. start-time( when the application that intens to use
                    SQLite is initaited

                    3. ru-time ( when a new SQLit datbase connection is
                    beign created

                NOTE: each threadh have to create its own connection to the
                database
                solution: acquire a ock to access the database direclty from
                 tyoru program.

                  here> what do i need to know before I understand how to write
                  sqlite ?
                     DO i eed asynchronous ?
                        asychroous vs pool?

            example
                put -> send data
                task_done -> signal that task is finished
                join -> join is used with taks_down

            demo
                1. add number into the database every 1 second,
                2. turn value to negative every 2 second.


        for twitter.
            here> implemet streaming data for twitter.

    figure out a way to run with following constraint.
        1. run twitter and reddit at the same time
        2. run sentiment on twitter data and reddit data


    applying sentiment
    stream data
    problem: there are delay when apply sentiment function


here> Run sentiment analysis on the crawled data.
    Are there anything else I need to crawled?


change styel so that I crawled all the data from twitter and do grouping
ourselves.
add sentiment to the current output data.
implement streaming data


crawler data using reddit crawler
    here> lets crawler 2 months of data
        submission
            figure out why submission is failing.
        comment
           here>  fix the code so that 84 data returns.

            use praw metadata to crawl data for the following, and check how
             many exists:
                no query
                1 query
                all query.

            save_all_data and its metadata too (depends on how many data do
                I get.)
                should
            here> figure out why all_full_day_response only return 3 when
            ask to
                rturn 1000?
                here> what is the len of generator when directly convert to
                list.
                    Note: I expect it to be 1000.
                why does not it execute inside for loop?
            implement save_file
            after debug stop, ans the following within 30 mins.
                check that all data is retrived for specified
                    max_response_cache, and repeat.
                    caches return whan all data is retrieved or return once
                    every 1 repeat?

                should I learn how retrying work in psaw?
                    why do i get 429 so often?
                    does it use proxy?

            create sqlite as datawarehouse, called "reddit_datawarehouse"
                how to create sqlite database?
            save data to datawarehouse database?

        How to crawl 2 month of data.

fix twitter crawler
    read issue from GetOldTweet3
        https://github.com/Mottl/GetOldTweets3/issues/98
    write test on twitter crawler and make necessary changes.

finish integrating request function (using psaw)
    note: I believe if I clearly stated what I need to do in the lastest log
    figure out why test failing.
        social_media_crawler NameError

get reddit dataset from it


create test on twitter crawler
    what do i need to crawl?
        What do i need to test?
